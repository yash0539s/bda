#!/usr/bin/env python
import sys
for line in sys.stdin:
    if line.startswith("X"):  # skip header
        continue
    parts = line.strip().split(',')
    if len(parts) > 8:
        month = parts[2]
        temp = float(parts[8])
        print(f"{month}\t{temp}")
 reducer.py
#!/usr/bin/env python
import sys
from collections import defaultdict

temps = defaultdict(list)
for line in sys.stdin:
    line = line.strip()
    if not line:
        continue
    month, temp = line.split('\t')
    temps[month].append(float(temp))

for month in temps:
    avg = sum(temps[month]) / len(temps[month])
    print(f"{month}\t{avg:.2f}")

cd D:\hadoop_project

run 
hdfs dfs -mkdir /input
hdfs dfs -put forestfires.csv /input/

Verify:

STEP 4Ô∏è‚É£ ‚Äî RUN HADOOP STREAMING

Now run the MapReduce job using Python files.

Find your streaming JAR file ‚Äî usually located at:

C:\hadoop\share\hadoop\tools\lib\hadoop-streaming-*.jar


Then run this command:

hadoop jar C:\hadoop\share\hadoop\tools\lib\hadoop-streaming-3.3.1.jar ^
-input /input/forestfires.csv ^
-output /output/fire_avg_temp ^
-mapper "python mapper.py" ^
-reducer "python reducer.py"


üî∏ If it says ‚Äúoutput directory already exists‚Äù, remove it first:

hdfs dfs -rm -r /output/fire_avg_temp

üìä STEP 5Ô∏è‚É£ ‚Äî VIEW RESULTS
hdfs dfs -cat /output/fire_avg_temp/part-00000

üêù STEP 6Ô∏è‚É£ (OPTIONAL) ‚Äî Run Hive for Data Mining

If Hive is installed and configured with Hadoop:

Open Hive shell:

hive


Run queries:

CREATE TABLE forestfires (
  X INT, Y INT, month STRING, day STRING,
  FFMC FLOAT, DMC FLOAT, DC FLOAT, ISI FLOAT,
  temp FLOAT, RH FLOAT, wind FLOAT, rain FLOAT, area FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA INPATH '/input/forestfires.csv' INTO TABLE forestfires;

SELECT month, SUM(area) AS total_area
FROM forestfires
GROUP BY month
ORDER BY total_area DESC
LIMIT 5;
